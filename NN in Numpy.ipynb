{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 1000\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "\n",
    "X, y = make_moons(n_samples = N_SAMPLES, noise=0.2, random_state=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[1], X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(nn_architecture, seed=99):\n",
    "    np.random.seed(seed)\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    params_values = {}\n",
    "\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "\n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backwards(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backwards(dA, Z):\n",
    "    #print(\"relu backwards shape\")\n",
    "    #print(dA.shape, Z.shape)\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_prop(A_prev, W_curr, b_curr, activation = \"relu\"):\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "\n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non supported activation function')\n",
    "        \n",
    "    A_curr = activation_func(Z_curr)\n",
    "\n",
    "    return A_curr, Z_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_forward_prop(X, params_values, nn_architecture):\n",
    "    memory = {}\n",
    "    A_curr = X\n",
    "\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        A_prev = A_curr\n",
    "        print(A_prev.shape)\n",
    "\n",
    "        activ_func_curr = layer[\"activation\"]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        print(W_curr.shape)\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        print(b_curr.shape)\n",
    "        A_curr, Z_curr = single_layer_forward_prop(A_prev, W_curr, b_curr, activ_func_curr)\n",
    "        \n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "\n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "\n",
    "def get_cost_value(Y_hat, Y):\n",
    "    m = Y_hat.shape[1]\n",
    "    cost = -1/m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1-Y, np.log(1-Y_hat).T))\n",
    "    #return np.squeeze(cost)\n",
    "\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return(Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_back_prop(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    \"\"\"\n",
    "    Implements dZ[l] = dA[l] * g' * (Z[l])\n",
    "    This is the backprop for one layer\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # number of examples\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        backward_act_func = relu_backwards\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_act_func = sigmoid_backwards\n",
    "    else:\n",
    "        raise Exception('Not supported activation function')\n",
    "    \n",
    "    # calculate derivative of activation function\n",
    "    dZ_curr = backward_act_func(dA_curr, Z_curr)\n",
    "    print(dZ_curr)\n",
    "    \n",
    "    return dZ_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_backward_prop(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    \"\"\"\n",
    "    Find gradients -((Y/Y_hat) - ((1-Y)/(1-Y_hat))) = dY_hat\n",
    "    \"\"\"\n",
    "    grads_values = {}\n",
    "    \n",
    "    # number of examples\n",
    "    m = Y.shape\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    dA_prev = -(np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat))\n",
    "    print(\"dA_prev = \\n\")\n",
    "    print(dA_prev.shape)\n",
    "    #print(dA_prev[:10])\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        # We extract all the parameters for each layer\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        print(\"Layer idx curr and prev = \", layer_idx_curr, layer_idx_prev)\n",
    "        activation_func_curr = layer['activation']\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        # run single layer backprop and update grad_values\n",
    "        \n",
    "        dZ = single_layer_back_prop(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation_func_curr)\n",
    "        \n",
    "        print(\"dZ = \")\n",
    "        print(dZ.shape)\n",
    "        #print(dZ[:10])\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    return print(\"End\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_architecture = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 5, \"activation\": \"relu\"},\n",
    "    #{\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    #{\"input_dim\": 50, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    #{\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 5, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, nn_architecture, epochs = 10, learning_rate = 0.01):\n",
    "    params_values = init_layers(nn_architecture)\n",
    "    print(\"initial params values = \\n\")\n",
    "    print(params_values)\n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # forward prop\n",
    "        y_hat, memory = full_forward_prop(X_train, #X_train.reshape(X_train.shape[1], X_train.shape[0]),\n",
    "                                          params_values, nn_architecture)\n",
    "        print(\"\\n y_hat shape = \\n\")\n",
    "        print(y_hat.shape)\n",
    "        \n",
    "        # Calc loss\n",
    "        loss = get_cost_value(y_hat, y_train)\n",
    "        print(\"\\n Loss = \\n\")\n",
    "        print(loss)\n",
    "        \n",
    "        \n",
    "        print(\"\\nNew params = \\n\")\n",
    "        print(params_values)\n",
    "        \n",
    "        # Back prop!\n",
    "        \n",
    "        full_backward_prop(y_hat, y_train, memory, params_values, nn_architecture)\n",
    "        \n",
    "\n",
    "    \n",
    "    return params_values, y_hat, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.transpose(y_train.reshape((y_train.shape[0], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial params values = \n",
      "\n",
      "{'W1': array([[-0.01423588,  0.20572217],\n",
      "       [ 0.02832619,  0.1329812 ],\n",
      "       [-0.01546219, -0.00690309],\n",
      "       [ 0.07551805,  0.08256466],\n",
      "       [-0.01130692, -0.23678376]]), 'b1': array([[-0.01670494],\n",
      "       [ 0.0685398 ],\n",
      "       [ 0.00235001],\n",
      "       [ 0.04562013],\n",
      "       [ 0.02704928]]), 'W2': array([[-0.14350081,  0.08828171, -0.05800817, -0.05015653,  0.05909533]]), 'b2': array([[-0.07316163]])}\n",
      "(2, 900)\n",
      "(5, 2)\n",
      "(5, 1)\n",
      "(5, 900)\n",
      "(1, 5)\n",
      "(1, 1)\n",
      "\n",
      " y_hat shape = \n",
      "\n",
      "(1, 900)\n",
      "\n",
      " Loss = \n",
      "\n",
      "0.4955555555555556\n",
      "\n",
      "New params = \n",
      "\n",
      "{'W1': array([[-0.01423588,  0.20572217],\n",
      "       [ 0.02832619,  0.1329812 ],\n",
      "       [-0.01546219, -0.00690309],\n",
      "       [ 0.07551805,  0.08256466],\n",
      "       [-0.01130692, -0.23678376]]), 'b1': array([[-0.01670494],\n",
      "       [ 0.0685398 ],\n",
      "       [ 0.00235001],\n",
      "       [ 0.04562013],\n",
      "       [ 0.02704928]]), 'W2': array([[-0.14350081,  0.08828171, -0.05800817, -0.05015653,  0.05909533]]), 'b2': array([[-0.07316163]])}\n",
      "dA_prev = \n",
      "\n",
      "(1, 900)\n",
      "Layer idx curr and prev =  2 1\n",
      "[[ 0.47890305 -0.51814476  0.47921195 -0.51567441 -0.5238744   0.48214369\n",
      "   0.48241444 -0.51715364 -0.5236257   0.48376413 -0.51930034  0.47732807\n",
      "  -0.514266    0.48261239 -0.51783045 -0.51805046 -0.51755634  0.48288674\n",
      "  -0.51730138  0.47766426  0.48090539  0.48416972  0.48302827  0.48012375\n",
      "  -0.52236397 -0.51654667 -0.52183335 -0.51576128  0.48287111 -0.52103043\n",
      "  -0.51563866  0.48035753 -0.52295916 -0.51734419 -0.52438418  0.48327824\n",
      "  -0.52325733  0.48401396 -0.5276581  -0.5176654  -0.5255904   0.4836014\n",
      "   0.4725907  -0.51888383 -0.52280122  0.48159035  0.48099774  0.47701601\n",
      "   0.47228114  0.48319642 -0.51759125 -0.52344768 -0.51396765 -0.51969197\n",
      "   0.4847541   0.47879864  0.47715715 -0.52161701  0.47929104 -0.52081879\n",
      "   0.48246894 -0.51688587 -0.51966819  0.48288873 -0.52765848  0.48040525\n",
      "   0.47960433  0.48425708  0.48300907 -0.52136377  0.48160719  0.48369622\n",
      "   0.47907635 -0.52047217 -0.51810416 -0.52139654 -0.52016934  0.47952561\n",
      "   0.47912568 -0.51974355 -0.51913272 -0.51683324  0.48071408  0.4834827\n",
      "   0.47193849  0.48128732  0.48266765  0.4832183   0.47921873  0.47860778\n",
      "   0.47340474  0.48303317  0.4837487  -0.51710376  0.47866946  0.47876008\n",
      "  -0.51411834 -0.51718958 -0.51879718  0.48361076 -0.52880847 -0.51692472\n",
      "  -0.51667737 -0.52340297  0.4830582   0.48274606  0.47609222  0.48358943\n",
      "  -0.51696832  0.47893267  0.47710582  0.48351912 -0.52680566  0.48257679\n",
      "  -0.5166076  -0.52174019  0.48311964 -0.51687643 -0.52007068  0.47681843\n",
      "   0.48002314 -0.5179604  -0.51695704  0.47715354 -0.5187919  -0.52092468\n",
      "   0.47629646 -0.5163684   0.48068066  0.4810373  -0.51803027  0.48317616\n",
      "  -0.52723019  0.48301315  0.47401903  0.48262399 -0.52283022 -0.51689821\n",
      "  -0.51646659  0.475548    0.48132616 -0.5220861  -0.51690028 -0.51708896\n",
      "   0.48319039  0.4781586   0.47909684 -0.5208069  -0.51702339 -0.52351823\n",
      "  -0.52478389  0.48338082  0.47418528 -0.51789423  0.48547805  0.47930163\n",
      "   0.47692531 -0.51631119  0.48320562  0.47769606 -0.52428052 -0.51648523\n",
      "   0.4833282  -0.52184059 -0.51750656  0.48322131 -0.5166817   0.47769437\n",
      "   0.48159055  0.47843431  0.48173541 -0.51809514 -0.52416839 -0.5170882\n",
      "   0.48184015 -0.51715367 -0.51718018  0.4793555  -0.52006197 -0.51631787\n",
      "   0.4837443  -0.51996952 -0.51872354 -0.51675287  0.47952388  0.48327321\n",
      "   0.47904055 -0.52035526  0.48234747 -0.51714868 -0.5147832  -0.51931351\n",
      "  -0.51576817  0.47875006  0.47127603  0.48006946 -0.52783958 -0.51684546\n",
      "  -0.52304676  0.48273551 -0.51741389 -0.51702918  0.48164887 -0.51649596\n",
      "   0.48408611  0.48322976  0.47854054  0.47898131 -0.52709413 -0.51737495\n",
      "  -0.52176563 -0.51683879 -0.52836504 -0.51706983 -0.52726132  0.48354618\n",
      "  -0.51674815  0.47844549 -0.51700233  0.48276516 -0.52780622  0.48045246\n",
      "   0.48595814  0.48233273 -0.52623224 -0.51723401 -0.51826741 -0.51727851\n",
      "  -0.52179924  0.47896625 -0.5169694   0.47807958 -0.51889128 -0.51671096\n",
      "   0.48225489 -0.51928986  0.47198709 -0.52016017  0.4739099  -0.51671086\n",
      "   0.48309956 -0.52102453 -0.52146471 -0.5154025  -0.51903577 -0.51666943\n",
      "  -0.52379734  0.48403134  0.48360256  0.48070943 -0.51655862 -0.52235489\n",
      "   0.47406836  0.48343636 -0.51756973  0.4822621   0.48561407 -0.51814109\n",
      "  -0.51602746 -0.51987497  0.47888978  0.48365453  0.48552939 -0.51960316\n",
      "  -0.52755668 -0.51688472 -0.52682184  0.4811654  -0.52469033 -0.51724583\n",
      "  -0.52142378  0.48308873 -0.51765956  0.48315471  0.47460494 -0.51644361\n",
      "  -0.51759933  0.48011154  0.48390281  0.48075179 -0.51585272  0.47707967\n",
      "   0.4820591   0.47752368  0.48430184  0.47884925  0.47778114 -0.51654349\n",
      "  -0.51969491  0.47882439  0.4838317   0.47958111 -0.51493866 -0.52083167\n",
      "  -0.52706054 -0.51874444 -0.51652413 -0.52343039 -0.51669447 -0.52017469\n",
      "   0.47021621 -0.51723625  0.47309826  0.48318164  0.4724069   0.48098576\n",
      "  -0.51340604 -0.5184657  -0.52089586  0.48133644 -0.51731809  0.48285043\n",
      "   0.47888138  0.48263748 -0.51987344 -0.52167335 -0.52598249 -0.51681153\n",
      "  -0.51722043  0.47715474  0.47613816 -0.51658592 -0.51955411  0.48324749\n",
      "  -0.52839633 -0.51922446  0.48261017 -0.52145439 -0.51766689  0.47758113\n",
      "   0.48337661 -0.52180291 -0.51486215 -0.52204104  0.47951434  0.48362177\n",
      "   0.47626141 -0.5174189  -0.51706171 -0.52355955 -0.52695701 -0.51955937\n",
      "  -0.52023059 -0.51808894  0.47848861  0.4822579   0.47678072  0.48344601\n",
      "  -0.51968782 -0.51698098 -0.52338157 -0.51634636  0.48524813  0.48050497\n",
      "  -0.52156326 -0.51703877 -0.51865007 -0.52183696  0.47793168 -0.52011705\n",
      "  -0.51620109 -0.52042192 -0.52149796  0.48049897  0.48030572 -0.52167577\n",
      "  -0.51928497  0.4832577   0.48264367  0.47916551  0.4834212  -0.52461508\n",
      "   0.47503377 -0.51698927  0.47757054  0.48400838 -0.51961387 -0.5166307\n",
      "   0.47306262 -0.5169509  -0.52588362  0.48421669 -0.52216228 -0.51888019\n",
      "   0.47143642  0.48281157  0.47756835 -0.51838589  0.4741282  -0.51699778\n",
      "  -0.52288639 -0.51662399  0.47497688 -0.51659738 -0.52164325 -0.5193955\n",
      "  -0.51645109  0.47836784 -0.51360518  0.48305817  0.47873849 -0.51904743\n",
      "   0.48126712  0.47882297 -0.5173877  -0.5204659   0.48244515  0.47838758\n",
      "   0.47755587  0.47977144 -0.52000156  0.47905986 -0.52617909  0.48309402\n",
      "  -0.52280391  0.48323364 -0.52752435 -0.51793235 -0.51644431 -0.5194855\n",
      "  -0.5177931   0.47713554  0.48292794  0.47782397 -0.51766937 -0.52060533\n",
      "   0.4857942  -0.5190171   0.4792699   0.4846294   0.48519671  0.48059147\n",
      "   0.47892883  0.48392547  0.48114684 -0.51706515 -0.52237887  0.48373571\n",
      "   0.47726672 -0.51936947  0.47310896  0.48173013 -0.51668697  0.48156462\n",
      "   0.47277274  0.48200762  0.48459647 -0.51843138 -0.51652455  0.47758157\n",
      "   0.48065694 -0.51876826  0.48234937 -0.52088001 -0.51671383  0.47752693\n",
      "   0.47874362 -0.51629447 -0.52012715  0.48442689 -0.51645032 -0.52136529\n",
      "  -0.52725872 -0.51722741  0.48254618  0.48342318  0.47826118 -0.51826046\n",
      "  -0.51633936 -0.51754311 -0.51773492 -0.52242582  0.48317446  0.47846841\n",
      "  -0.5152343   0.47842759  0.47263253  0.48088518  0.47940483  0.48373813\n",
      "  -0.51680631 -0.52266436 -0.51780905 -0.52130903  0.48219987 -0.51743802\n",
      "   0.47872477  0.48367655  0.47893767  0.483391   -0.51917128 -0.51692552\n",
      "  -0.51685189 -0.52242057  0.48091008 -0.51827073  0.48202642  0.48299585\n",
      "   0.47666468 -0.51746274 -0.5171047   0.48150174 -0.51939577 -0.52010372\n",
      "   0.47597515 -0.51681367 -0.51701808  0.48263928 -0.51571454  0.47910026\n",
      "  -0.51489337  0.48160142  0.48344744 -0.52133439  0.47874373 -0.51997188\n",
      "  -0.52218063 -0.517055   -0.52265069  0.48381573  0.47777143 -0.51929363\n",
      "   0.48407485  0.48108542  0.47291925 -0.51704762 -0.5214312   0.47957992\n",
      "  -0.5188536   0.48325686 -0.52618085  0.48286971 -0.51531578  0.47907075\n",
      "   0.47091448 -0.51709815 -0.51773609 -0.51729545  0.47802724 -0.5182028\n",
      "   0.48493087 -0.51847778 -0.52607544  0.48284442 -0.51944831  0.48068491\n",
      "   0.48237954 -0.51705224 -0.52173044  0.48081373 -0.51461658 -0.51839441\n",
      "   0.47894829  0.47820667 -0.52341212 -0.51749962 -0.52326025 -0.51705053\n",
      "   0.47938091 -0.52251846 -0.51726161  0.48328916 -0.52523474 -0.51732067\n",
      "   0.48472296  0.48254397 -0.51390153 -0.51721106 -0.51788996 -0.51716456\n",
      "   0.4783208  -0.51625275 -0.51685663 -0.52363023  0.48332426  0.47921432\n",
      "  -0.51655908 -0.51871637  0.47740409  0.48219947 -0.52436677  0.48337265\n",
      "  -0.52936245 -0.51899829  0.48171539 -0.5158373  -0.51761033 -0.52035822\n",
      "  -0.51831835  0.47963416  0.4795238   0.47945455  0.48291213 -0.52003732\n",
      "   0.48346718  0.47924504  0.47228294 -0.51700566  0.48154781  0.47966861\n",
      "  -0.51762205 -0.52144755 -0.51744292  0.48342927  0.48514591 -0.51687302\n",
      "   0.48022679  0.48347434 -0.516795    0.47816195 -0.52331648 -0.51738392\n",
      "  -0.51678266  0.47722029  0.48338945  0.47816121  0.48367354 -0.52065063\n",
      "  -0.52760244  0.48296012 -0.52111271  0.4797267   0.47946739 -0.51707547\n",
      "  -0.51675475 -0.52032566  0.48260992 -0.51694889  0.47726986  0.48191675\n",
      "   0.48241911  0.47997904  0.47492188 -0.52167028  0.48325963  0.48001934\n",
      "  -0.51883206 -0.5197102  -0.51690403 -0.52036017 -0.51805844  0.48274521\n",
      "  -0.52390457  0.48358052 -0.52694023 -0.51668929 -0.51659272  0.47865992\n",
      "   0.48271113  0.48281699  0.48249846  0.48304515  0.48187187  0.48036131\n",
      "   0.4847128   0.48119444 -0.52082008 -0.51659712 -0.52707074  0.48307705\n",
      "   0.48162375  0.47882824  0.48247923  0.48139753 -0.51854689  0.48330297\n",
      "  -0.52183401 -0.51975099 -0.51674502  0.47772983 -0.52362992 -0.51699849\n",
      "  -0.51669707  0.47880411  0.48358266  0.48033701  0.48365272  0.48056284\n",
      "  -0.5169576  -0.51784495  0.48169331 -0.5192547  -0.51727399  0.48144633\n",
      "  -0.51401201  0.48309272  0.48168536  0.47795914 -0.5221123  -0.51474025\n",
      "   0.4816649   0.48071466 -0.5228029  -0.51778508 -0.52131137 -0.51712199\n",
      "   0.4799726  -0.51654183 -0.52461051 -0.51623178 -0.51469163  0.47904025\n",
      "  -0.51595823 -0.52049913  0.47845728  0.48239608  0.48428698  0.47835718\n",
      "   0.47097629 -0.51846445  0.47959425  0.48077917 -0.51631596  0.47610346\n",
      "  -0.51627951 -0.52239204  0.47899832 -0.51857736 -0.52801477  0.48194947\n",
      "   0.47632688 -0.51503829  0.48352103 -0.52210341  0.48287261  0.47791004\n",
      "   0.47187719  0.48302553 -0.52255582  0.48329815 -0.51759741 -0.51926145\n",
      "   0.47370509  0.48335674 -0.51680414 -0.52138465  0.48410096  0.4798396\n",
      "   0.48164834  0.48353879 -0.5190689  -0.51613612 -0.51663581  0.47974313\n",
      "   0.47319023  0.4827934  -0.51488329  0.47918753 -0.51869749  0.48326038\n",
      "   0.47729533 -0.51574483  0.48425338 -0.51687101  0.48159601  0.4811116\n",
      "   0.47938444  0.48212294  0.48088871 -0.52261391  0.48079094  0.48329731\n",
      "   0.48494037 -0.51994479  0.48315834  0.47830306 -0.51529125  0.47882399\n",
      "  -0.52087775  0.48325206 -0.51857802 -0.51663786 -0.51644949  0.48151896\n",
      "  -0.5233625  -0.51565446 -0.51688572  0.47762311 -0.51965737 -0.51699859\n",
      "   0.48245936 -0.51939378 -0.52731998  0.48317728  0.47986037  0.47889167\n",
      "   0.47955747  0.48388012  0.47769937 -0.51783475 -0.51704189 -0.52116584\n",
      "   0.47904004  0.48215231 -0.51421286  0.48245976  0.48253676 -0.52238662\n",
      "  -0.51648895 -0.52233013  0.47338121  0.4827034   0.47415643  0.48356676\n",
      "  -0.52034603  0.48342607  0.47886969  0.47821219  0.47832049 -0.51786602\n",
      "  -0.5197393   0.48333976  0.48343741 -0.5222367  -0.5264327   0.48133689\n",
      "  -0.52029222  0.48003134  0.47732768 -0.51639809 -0.52458464  0.48332474\n",
      "  -0.51671414  0.48260342 -0.51994915 -0.52293928  0.48028233  0.48197718\n",
      "   0.47274637  0.48065932 -0.51886965  0.47946491  0.47476889  0.48319077\n",
      "  -0.51794982 -0.5204076  -0.51715742 -0.51830517  0.47655478 -0.51620318\n",
      "  -0.5151822  -0.51815513  0.48141962 -0.52181268  0.47823097  0.48326194\n",
      "   0.4786726   0.47835514  0.48369871  0.48282441 -0.51641633 -0.52083183\n",
      "  -0.52263957  0.48308633  0.47821186 -0.5162025   0.48294032 -0.51807719\n",
      "   0.48429335 -0.52191446 -0.52141293 -0.5197294   0.472274    0.48265076\n",
      "  -0.5153936   0.48094468 -0.51658565 -0.52190059 -0.51719091 -0.51707755\n",
      "   0.48226043  0.48321657 -0.51403201  0.4834      0.47758099  0.48338079\n",
      "  -0.51808197 -0.52100901 -0.52559549  0.48378786  0.47187512 -0.51675672\n",
      "   0.47630117  0.48326316 -0.52878009 -0.51703251  0.47761066  0.48307832\n",
      "   0.48178505 -0.51899928 -0.51742145 -0.52239584 -0.52689676 -0.51782281\n",
      "  -0.51695824  0.47708418 -0.51674463  0.47719359  0.48269368  0.47853619]]\n",
      "dZ = \n",
      "(1, 900)\n",
      "Layer idx curr and prev =  1 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 1 but corresponding boolean dimension is 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-80570bfe7a5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnn_architecture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-b54a1026ab5d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X_train, y_train, nn_architecture, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Back prop!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mfull_backward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-92d38c2f615f>\u001b[0m in \u001b[0;36mfull_backward_prop\u001b[1;34m(Y_hat, Y, memory, params_values, nn_architecture)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# run single layer backprop and update grad_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mdZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msingle_layer_back_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA_curr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_curr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_curr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ_curr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_func_curr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dZ = \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-93054f691545>\u001b[0m in \u001b[0;36msingle_layer_back_prop\u001b[1;34m(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# calculate derivative of activation function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mdZ_curr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackward_act_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA_curr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ_curr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ_curr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-97d3627363a1>\u001b[0m in \u001b[0;36mrelu_backwards\u001b[1;34m(dA, Z)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#print(dA.shape, Z.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mZ\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 1 but corresponding boolean dimension is 5"
     ]
    }
   ],
   "source": [
    "params, y_hat , memory = train(X_train=X_train, y_train=y_train, nn_architecture=nn_architecture, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = y_hat.shape[1]\n",
    "cost = -1/m * (np.dot(y_train, np.log(y_hat).T) + np.dot(1-y_train, np.log(1-y_hat).T))\n",
    "np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "454/900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in reversed(list(enumerate(nn_architecture))):\n",
    "    print(idx)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i['activation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(memory):\n",
    "    print(memory[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu(-3), relu(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
